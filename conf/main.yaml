defaults:
  # the name of the algorithm to be used ('td3', 'sac', 'dqn', 'ddpg', 'her2', 'hac')
  # here we use hydras config group defaults
  - algorithm: 'hac'
  - override hydra/job_logging: default
  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: tpe
  - override hydra/launcher: custom_joblib # For multiprocessing, allows for n_jobs > 1. Comment this line to use the standard launcher which spawns a single process at a time.


# The name of the OpenAI Gym environment that you want to train on.
env: 'AntReacher-v1'
#env: 'ButtonUnlock-o1-v1'
#env: 'FetchReach-v1'
# Currently supported envs:
# 'FetchPush-v1',
# 'FetchSlide-v1',
# 'FetchPickAndPlace-v1',
# 'FetchReach-v1',

# 'HandManipulateBlock-v0',
# 'Hook-o1-v1',
# 'ButtonUnlock-o2-v1',
# 'ButtonUnlock-o1-v1',

# 'AntReacher-v1',
# 'Ant4Rooms-v1',
# 'AntMaze-v1',
# 'AntPush-v1',
# 'AntFall-v1',

# 'BlockStackMujocoEnv-gripper_random-o0-v1',
# 'BlockStackMujocoEnv-gripper_random-o2-v1',
# 'BlockStackMujocoEnv-gripper_above-o1-v1',
# 'BlockStackMujocoEnv-gripper_none-o1-v1',


seed: 0

# layer classes for the algorithm
layer_classes:
  - sacvg
  - sacvg

# the path to where logs and policy pickles should go.
base_logdir: 'data'

# The pretrained policy file to start with to avoid learning from scratch again. Useful for interrupting and restoring training sessions.
restore_policy: null

# The number of training steps after which to evaluate the policy.
eval_after_n_steps: 2000

# The max. number of training epochs to run. One epoch consists of 'eval_after_n_steps' actions.
n_epochs: 75

# The number of testing rollouts.
n_test_rollouts: 10

# Max. number of tries for this training config.
max_try_idx: 399

# Index for first try.
try_start_idx: 100

# The n last epochs over which to average for determining early stopping condition.
early_stop_last_n: 3

# The early stopping threshold.
early_stop_threshold: 0.9

# The data column on which early stopping is based.
early_stop_data_column: 'test/success_rate'

# A command line comment that will be integrated in the folder where the results
# are stored. Useful for debugging and addressing temporary changes to the code..
info: ''

# Max. number of tensorboard instances allowed at the same time.
# Will be determined by number of open ports, starting at port 6006
tensorboard: 2

# Data to plot for evaluation. Strings separated by comma.
plot_eval_cols:
 - 'test/success_rate'
 - 'test/mean_reward'
 - 'test_0/ep_length'

# Min. number of seconds to wait for next plot with MatplotlibOutputFormat. OBSOLETE since the matplotlib logger is not used any more.
plot_at_most_every_secs: 360

# The number of steps after which to save the model. 0 to never save, i.e., to only save the best and last model.
save_model_freq: 0

# TODO: Currently, having a subfolder conf/hydra/output is buggy
# override default dirname config
hydra:
  run:
    # add git commit hash
    dir: ${base_logdir}/${git_label:}/${env}/${now:%H-%M-%S}
  sweep:
    dir: ${base_logdir}/${git_label:}/${env}/${now:%H-%M-%S} # This way, all trials within one hyperopt run are stored in a subfolder determined by the current time.
#    dir: ${base_logdir}/${git_label:}/${env} # This way, all trials within multiple hyperopt runs are stored in the same parent folder, without using the time subfolder.
    subdir: ${hydra.job.num}

#  launcher: # Note that the joblib launcher does not work with comet_ml, hence multiprocessing within python does not work with comet_ml.
#    _target_: hydra_plugins.hydra_custom_joblib_launcher.joblib_launcher.JoblibLauncher

  # sweeper, sampler and search_space is for hyperparameter optimization. Optimization can be started with --multirun as commandline parameter when executing train.py
  sweeper:
    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 123
      consider_prior: true
      prior_weight: 1.0
      consider_magic_clip: true
      consider_endpoints: false
      n_startup_trials: 10
      n_ei_candidates: 24
      multivariate: false
      warn_independent_sampling: true
    _target_: hydra_plugins.hydra_custom_optuna_sweeper.custom_optuna_sweeper.CustomOptunaSweeper
    direction: maximize
    study_name: hac_2_layer_button_o1_opti_critic
    storage: sqlite:///hac_2_layer_button_o1_opti_critic.db
    n_jobs: 9
    max_trials: 9999
    max_duration_minutes: 10000
    min_trials_per_param: 3
    max_trials_per_param: 9

    search_space:

#      algorithm.opti_normalized_critic:
#        type: categorical
#        choices:
#          - 0
#          - 1

      algorithm.learning_rates.0:
        type: float
        low: 0.0005
        high: 0.007
        log: true

      algorithm.learning_rates.1:
        type: float
        low: 0.0005
        high: 0.007
        log: true

#      algorithm.time_scales.0:
#        type: int
#        low: 5
#        high: 10
#        step: 1
#
#      algorithm.time_scales.1:
#        type: int
#        low: 5
#        high: 10
#        step: 1

      algorithm.n_sampled_goal:
        type: int
        low: 2
        high: 10
        step: 2

      algorithm.subgoal_test_perc:
        type: float
        low: 0.0
        high: 0.6
        step: 0.1

      algorithm.goal_selection_strategy:
        type: categorical
        choices:
          - future
          - future2
          - rndend
          - rndend2

#      algorithm.use_action_replay:
#        type: categorical
#        choices:
#          - 1
#          - 0

#      algorithm.ep_early_done_on_succ:
#        type: int
#        low: 0
#        high: 1
#        step: 1

#      algorithm.hindsight_sampling_done_if_success:
#        type: categorical
#        choices:
#          - 1
#          - 0

#      algorithm.set_fut_ret_zero_if_done:
#        type: categorical
#        choices:
#          - 1
#          - 0