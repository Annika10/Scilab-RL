defaults:
  # the name of the algorithm to be used ('td3', 'sac', 'dqn', 'ddpg', 'her2', 'hac')
  # here we use hydras config group defaults
  - algorithm: 'hac'
  - override hydra/job_logging: default
  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: tpe
  - override hydra/launcher: custom_joblib # For multiprocessing, allows for n_jobs > 1. Comment this line to use the standard launcher which spawns a single process at a time. The standard launcher is much better for debugging.

# The name of the OpenAI Gym environment that you want to train on.
#env: 'Blocks-o1-gripper_random-v1'
#env: 'AntReacher-v1'
#env: 'ButtonUnlock-o1-v1'
env: 'FetchReach-v1'
#env: 'AntMaze-v0'
# Currently supported envs:
# 'FetchPush-v1',
# 'FetchSlide-v1',
# 'FetchPickAndPlace-v1',
# 'FetchReach-v1',

# 'HandManipulateBlock-v0',
# 'Hook-o1-v1',
# 'ButtonUnlock-o2-v1',
# 'ButtonUnlock-o1-v1',

# 'AntReacher-v1',
# 'Ant4Rooms-v1',
# 'AntMaze-v0',
# 'AntPush-v0',
# 'AntFall-v0',

# 'BlockStackMujocoEnv-gripper_random-o0-v1',
# 'BlockStackMujocoEnv-gripper_random-o2-v1',
# 'BlockStackMujocoEnv-gripper_above-o1-v1',
# 'BlockStackMujocoEnv-gripper_none-o1-v1',

seed: 0

# the path to where logs and policy pickles should go.
base_logdir: 'data'

# The pretrained policy file to start with to avoid learning from scratch again. Useful for interrupting and restoring training sessions.
restore_policy: null

# The number of training steps after which to evaluate the policy.
eval_after_n_steps: 2000

# The max. number of training epochs to run. One epoch consists of 'eval_after_n_steps' actions.
n_epochs: 60

# The number of testing rollouts.
n_test_rollouts: 10

# Max. number of tries for this training config.
max_try_idx: 399

# Index for first try.
try_start_idx: 100

# The n last epochs over which to average for determining early stopping condition.
early_stop_last_n: 3

# The early stopping threshold.
early_stop_threshold: 0.9

# The data column on which early stopping is based.
early_stop_data_column: 'test/success_rate'

# A command line comment that will be integrated in the folder where the results
# are stored. Useful for debugging and addressing temporary changes to the code..
info: ''

# The number of steps after which to save the model. 0 to never save, i.e., to only save the best and last model.
save_model_freq: 0

# Whether and how to render the rollout execution during training.
# \'record\' is for video, \'display\' for direct visualization, \'null\' is for not rendering at all.
render_train: null

# Whether and how to render the rollout execution during testing.
# \'record\' is for video, \'display\' for direct visualization, \'null\' for not rendering at all.
render_test: null

# TODO: Currently, having a subfolder conf/hydra/output is buggy
# override default dirname config
hydra:
  run:
    # add git commit hash
    dir: ${base_logdir}/${git_label:}/${env}/${now:%H-%M-%S}
  sweep:
    dir: ${base_logdir}/${git_label:}/${env}/${now:%H-%M-%S} # This way, all trials within one hyperopt run are stored in a subfolder determined by the current time.
#    dir: ${base_logdir}/${git_label:}/${env} # This way, all trials within multiple hyperopt runs are stored in the same parent folder, without using the time subfolder.
    subdir: ${hydra.job.num}

  # sweeper, sampler and search_space for hyperparameter optimization. Hyperparameter optimization can be started with --multirun as commandline parameter when executing train.py
  sweeper:
    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 123
      consider_prior: true
      prior_weight: 1.0
      consider_magic_clip: true
      consider_endpoints: false
      n_startup_trials: 10
      n_ei_candidates: 24
      multivariate: false
      warn_independent_sampling: true
    _target_: hydra_plugins.hydra_custom_optuna_sweeper.custom_optuna_sweeper.CustomOptunaSweeper
    direction: maximize
    # study_name: ${defaults.algorithm/name}_${defaults.algorithm/layer_classes}_${env}
    # storage: sqlite:///${defaults.algorithm/name}_${defaults.algorithm/layer_classes}_${env}.db
    # TODO: get more parameters out of the default config group
    study_name: ${defaults[0].algorithm}_${env}
    storage: ${defaults[0].algorithm}_${env}://${env}.db
    n_jobs: 6
    max_trials: 9999
    max_duration_minutes: 10000
    min_trials_per_param: 3
    max_trials_per_param: 9

    search_space:
      algorithm.learning_rates.0:
        type: float
        low: 0.0015
        high: 0.005
        log: true

      algorithm.learning_rates.1:
        type: float
        low: 0.0002
        high: 0.0007
        log: true

      algorithm.time_scales.0:
        type: int
        low: 15
        high: 25
        step: 10

      algorithm.time_scales.1:
        type: int
        low: 15
        high: 25
        step: 10

      algorithm.n_sampled_goal:
        type: int
        low: 2
        high: 10
        step: 2

      algorithm.subgoal_test_perc:
        type: float
        low: 0.0
        high: 0.6
        step: 0.2

      algorithm.goal_selection_strategy:
        type: categorical
        choices:
          - future
          - future2
          - rndend
          - rndend2

      algorithm.use_action_replay:
        type: categorical
        choices:
          - 1
          - 0

      algorithm.ep_early_done_on_succ:
        type: int
        low: 0
        high: 1
        step: 1

      algorithm.hindsight_sampling_done_if_success:
        type: categorical
        choices:
          - 1
          - 0

      algorithm.set_fut_ret_zero_if_done:
        type: categorical
        choices:
          - 1
          - 0
