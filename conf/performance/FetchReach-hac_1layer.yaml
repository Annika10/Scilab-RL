# @package _global_
# Changes specified in this config should be interpreted as relative to the _global_ package.
defaults:
  # access algorithm config group to get the default paramters of hac
  - override /algorithm: hac

# overwrite parameters of main.yaml
env: 'FetchReach-v1'

# The n last epochs over which to average for determining early stopping condition.
early_stop_last_n: 4

performance_testing_conditions:
  # In 2 out of 3 tests, the test/success rate should be at least 0.9 after 14000 steps.

  total_runs: 3 # How many runs in total:

  succ_runs: 2 # This number of runs should meet the conditions:

  eval_columns: test/success_rate # This is what we evaluate to determine success. Will use this to override the \'early_stop_data_column\' parameter of main.yaml

  eval_value: 0.9 # This is the value we determine for success. Will use this to determine and override the \'early_stop_threshold\' parameter of main.yaml

  max_steps: 14000 # This is the time limit for checking the success. Will use this and the \'eval_after_n_steps\' parameter of main.yaml to determine the n_epochs parameter in main.yaml.


algorithm:
  # layer classes for the algorithm
  layer_classes:
    - sacvg

  # number of hindisght goals.
  n_sampled_goal: 7

  # The learning rates of the layers. From highest to lowest layer.
  learning_rates:
    - 0.0018

  # Whether to consider an episode as done *in hindsight* during goal replay
  # if the hindsight goal has been achieved. This is important e.g. in SAC,
  # where the discounted expected return is set to 0 of an episode is done.
  # Only relevant if set_fut_ret_zero_if_done = 1.
  hindsight_sampling_done_if_success: 1

  # Steps per level from highest to lowest, separated by comma. There must be one -1
  # in the list to indicates the layer where the time scale is determined from the
  # environment's predefined steps.
  time_scales:
    - -1

hydra:
  sweeper:
    study_name: hac_1_layer_FetchReach-v1
    storage: sqlite:///hac_1_layer_FetchReach.db
    search_space:
      algorithm.learning_rates.0:
        type: float
        low: 6e-4
        high: 3e-2
        log: true
#
      algorithm.n_sampled_goal:
        type: int
        low: 1
        high: 8
        step: 1
#
      algorithm.subgoal_test_perc:
        type: float
        low: 0.0
        high: 0.7
        step: 0.1

      algorithm.goal_selection_strategy:
        type: categorical
        choices:
          - future
          - future2
          - rndend
          - rndend2

      algorithm.use_action_replay:
        type: categorical
        choices:
          - 1
          - 0

      algorithm.ep_early_done_on_succ:
        type: int
        low: 0
        high: 2
        step: 1

      algorithm.hindsight_sampling_done_if_success:
        type: categorical
        choices:
          - 1
          - 0

      algorithm.set_fut_ret_zero_if_done:
        type: categorical
        choices:
          - 1
          - 0